---
title: "第１章: エキスパート統合問題"
---
# Chapter 1 エキスパート統合問題

オンライン予測問題の導入として最も基本的な枠組みであるエキスパート統合問題について述べる

- ユニバーサル符号における情報源の確率分布推定問題や、繰り返しゲームにおける行動選択問題など、様々な領域における意思決定問題を表すことができるという一般性を持ち合わせる

## 1.1 N人のクイズ王の問題

考える問題設定は以下のとおり

**Problem Definition**
各試行 $t=1, \ldots T$ において以下の３つの処置が行われる

1. 環境はベクトル $\bm{z}_t=(z_{t, 1}, \ldots, z_{t, N})\in\{0, 1\}^N$ をプレイヤーに提示
2. プレイヤーは答え $x_t\in\{0, 1\}$ と予測する
3. 環境はクイズの正解 $y_t\in\{0, 1\}$ をプレイヤーに提示する

この環境において、誤り回数 $\sum_t|x_t-y_t|$ をできるだけ少なくしたい

記法および用語は以下のとおり

- プレイヤー：予測する人
- 環境：プレイヤーによって操作できないその他の要素
- エキスパート：以下に定義する $\bm{z}_t$ を生成してくれる存在
- $N:$  エキスパートの数
- $S_t=((\bm{z}_1, y_1), \ldots, (\bm{z}_{t-1}, y_{t-1}), \bm{z}_t):$  試行 $t$ において既に持ち合わせている情報の全て

解決策の最もシンプルなものは統計的学習の手法を利用すること

- 何らかの仮説クラス $H\subseteq \{h:\{0, 1\}^N\to\{0, 1\}\}$ を導入して $x_t=h_t(\bm{z}_t)$ のように推定する
- これは典型的な２値分類の問題
- しかし、これがうまくいく（汎化誤差が $0$ に収束する）にはサンプルの生成のされ方に関する確率的な仮定が必要である

**Assumption**
$\{0, 1\}^N\times\{0, 1\}$ 上のある分布 $D$ が存在して、各事例 $(\bm{z}_t, y_t)$ はそれぞれ独立に同分布 $D$ に従って生成される

一方で、サンプルの生成のされ方に上記のような仮定を含む任意の仮定を置かなければ、どれだけ優れたプレイヤーのアルゴリズムを用いても最悪の場合は全問不正解になることがありうる

- これはプレイヤーのアルゴリズム $A$ に基づいて、環境側のアルゴリズム $B$ が構築されていた場合
- これは非常に理不尽であるが、運命的に同じ振る舞いをしてしまう可能性も否定できない
- このような最悪の場合の解析を行う手法は**敵対的論法** (adversarial argument) と呼ばれる

## 1.2 全問正解のエキスパートが存在する場合

ある $i^\star$ 番目のエキスパートの推定値に関して $z_{t, i^\star}=y_t$ が成立しているケースを考える

### 1.2.1 ２分法

以下の戦略が良さそうに考えられる

> **全問正解を続けているエキスパートの多数派に従う**
>
- もしプレイヤーが不正解であった場合、半分以上の候補を削ることができる
- ここから２分法の誤り回数は高々 $\log_2 N$ に落ち着くことが導かれる

### 1.2.2 乱択２分法

次のような戦略を考える

> 全問正解を続けているエキスパートから一様ランダムに１人選び、その決定に従う
>
- この方法の誤り回数の期待値は $\ln N$ となり、通常の２分法よりも誤り回数の期待値が小さくなる

### 1.2.3 c乱択２分法

はじめに２分法および乱択２分法において、試行 $t$ で $x_t=1$ を出力する確率 $p_t$ について考察する

- Notations
    - $W_t$ を試行 $t-1$ までの全てのクイズに正解したエキスパートの数
    - $K_t$ をそれらのエキスパートの中で試行 $t$ のクイズで答えを $1$ と予測したエキスパートの数
- 以下の関数 $f$ を用いて確率 $p_t=f(W_t/K_t)$ のように表すことを考える
    - ２分法：$W_t/K_t$ が $1/2$ より大きいかどうかで判断する
    - 乱択２分法：$W_t/K_t$ の確率で $1$ を選ぶはずなので $f(x)=x$ のような形になるはず

期待値的には乱択２分法の方が良いものの、$W_t/K_t$ が限りなく $1$ に近いときは決定的に選ぶ２分法の方が良さそうに思える

- この仮定が正しいかどうかを以下のような一般化された関数 $f_c$ を用いて議論する

    $$
    f_c=\left\{
    \begin{align*}
    &0\quad(x<c) \\
    &x\quad(c\leq x < 1-c) \\
    &1\quad(1-c\leq x)
    \end{align*}
    \right.
    $$

- このような問題設定において $c$ が

    $$
    c=\frac{1-\ln2}{2}\approx0.15
    $$

    をみたすとき $c$ 乱択２分法が最適であり、その時の誤り回数の期待値は $(1/2)\log_2N$ となる

    - この結論から、エキスパートの約 $85\%$ 以上の答えが一致していればその答えに従った方が良いと言える

また、最適な $c$ 乱択２分法は全ての乱択アルゴリズムの中で最適であることが示せる

- これはプレイヤーの任意の乱択アルゴリズム $A$ に対して、誤り回数の期待値を $(1/2)\log_2N$ としつつ、全問正解のエキスパートの存在を保証するような敵対者のアルゴリズムが存在するから

## 1.3 全問正解のエキスパートが存在するとは限らない場合

全問正解のエキスパートが存在するという仮定はやはり極端であるので、そうでない場合を考えたい

- 📒 しかし、乱択なプレイヤーに対しても都合の良い敵対者のアルゴリズムを用いれば、誤り回数の期待値を $T/2$ 以上とすることができる
- 以上より、環境からの情報を用いないランダムな選択が最も優れているという結論が得られてしまう
- しかし、この結論も $N$ 人のエキスパートの振る舞いも含めて全て最悪な場合を想定しており、この問題設定自体が極論と言わざるを得ない
    - エキスパートも間違えまくっている環境自体かなり事故なので、それまで考慮に入れるのは如何のものか？

### 1.3.1 リグレット

現実的な問題設定としては以下のようなものだと考えられる

> $N$ 人のエキスパートのいずれかがそれなりに良い成績を達成している場合、それなりに良い成績をプレイヤーも達成するためのアルゴリズムは何か？
>
- それゆえに、今後の評価指標は「最も成績の良かったエキスパートの誤り回数と比較する」ことで相対的に評価するするような指標を用いる
- この評価指標はリグレット (regret) と呼ばれる

最後に、このN人のクイズ王の問題におけるリグレットの定義を与える

**Definition**
$N$ 人のエキスパートの問題に対する乱択アルゴリズム $A$
 に対し、敵対者が生成したデータ系列を $S=((\bm{z}_1, y_1), \ldots, (\bm{z}_{T}, y_{T}))$ とする。この時、アルゴリズム $A$
 のデータ系列 $S$ に対するリグレットを

$$
\text{Regret}_A(S)=\mathbb{E}\left[\sum_{t=1}^T|x_t-y_t|\right]-\min_{1\leq i\leq N}\sum_{t=1}^T|z_{t, i}-y_t|,
$$

また、総試行回数 $T$
 に対するリグレットを

$$
\text{Regret}_A(T)=\min_{S\in(\{0,1\}^N\times\{0,1\})^T}\text{Regret}_A(S)
$$

と定義する

### 1.3.2 リグレットの意味について

一言でリグレットの意味をいうと「最も優秀な人と比べてこのくらい誤ってしまったなぁ」の量を定量化したもの

- ここで、プレイヤーの選択 $x_t$ が次以降の敵対者の選択に影響を及ぼす可能性があることを考慮すると、ここで得られる数値はあくまでも仮想的なものであると言える
    - これは株式投資の場合を考えるとわかりやすい
- また、最優秀エキスパートが非常に成績が悪い場合は諦めるという立場をとっている
    - バッチ処理でも無理なんやったらオンライン処理なんてできるわけないやんという感じ

### 1.3.3 重み付き平均アルゴリズム

乱択２分法を微修正することでリグレットが小さいアルゴリズムが得られる

- エキスパート $i$ が試行 $t-1$ まで全問正解の時 $1$ を取るような指示変数 $w_{t, i}$ を用いていた
    - これに対して更新式のようなものを定義すると以下のようになる

        $$
        w_{t+1, i}=\left\{
        \begin{align*}
        &w_{t, i}\quad &y_t=z_{t, i} \\
        &0\quad &y_t\neq z_{t, i}
        \end{align*}
        \right.
        $$

- ここで、不正解の時を一気に $0$ にしてしまうのではなく $0<\beta<1$ の適当な値を取る $\beta$ とすることでリグレットが小さいアルゴリズムが得られる
    - このようなアルゴリズムを重み付き平均アルゴリズム (weighted averaging algorithm; WAA) と呼ぶ
    - もちろん $\beta=0$ の場合が乱択２分法である
- プレイヤーが $1$ を選ぶ確率は $p_t=\sum w_i z_{t,i}/W_t$ と書けるので、誤り回数の期待値の上界が導出できる

### 1.3.4 改良版重み付き平均アルゴリズム c-WAA

WAA に対して変換関数 $f_c$ を適用すればよりタイトな誤り回数の期待値の上界が得られる

### 1.3.5 パラメータ $\beta$ の選択とWAAのリグレット上界の導出

### 1.3.6 $\beta$ の自動調整 – ダブリング・トリック

## 1.4 エキスパート統合問題

### 1.4.1 N人のクイズ王の問題

### 1.4.2 オンライン配分問題

### 1.4.3 重みつき平均アルゴリズム

### 1.4.4 対数損失と情報圧縮

## 1.5 重みと損失関数による定式化

### 1.5.1 エキスパート統合問題の標準系

### 1.5.2 オンライン配分問題とヘッジアルゴリズム

### 1.5.3 重みと損失関数による定式化

### 1.5.4 第２章への準備
