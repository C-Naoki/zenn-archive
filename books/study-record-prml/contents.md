---
title: "全体像"
---
# 全体像
各章に関するあらましを以下の表にまとめました。

|  タイトル  | あらまし |
|:---:|:---:|
| **第１章: 序論** |  |
| 1.1: 多項式曲線フィッティング | 多項式を用いたデータへのフィッティングを考える |
| 1.2: 確率論 | 不確実性に関する定量化と操作に関して一貫した枠組みを与える |
| 1.2.1:  確率密度 | 離散的な事象集合に定義される確率分布のように<br>連続変数に対しても同様に確率というものを考えたい |
| 1.2.2: 期待値と分散 | 確率を含む最も重要な操作の内１つである関数の重み付き平均について |
| 1.2.3: ベイズ確率 | ベイズ的考え方を用いることで不確実性を定量化したい |
| 1.2.4: ガウス分布 | 様々な確率分布の中で最も重要なガウス分布を導入する |
| 1.2.5: 曲線フィッティング再訪 | i) 誤差関数及び正則化に関する洞察を得る, ii)完全なベイズ的取り扱いを知る, という目的を達成するために多項式曲線フィッティングを眺め直す |
| 1.2.6: ベイズ曲線フィッティング | 完全なベイズアプローチを考える |
| 1.3: モデル選択 | 実際に応用する際に決定する必要がある<br>モデルの複雑さを支配するパラメータについて |
| 1.4: 次元の呪い | パターン認識テクニックの設計に重要な影響を与える要因である<br>高次元空間の取り扱いについて |
| 1.5: 決定理論 | パターン認識で遭遇する不確かさを含む状況における<br>最適な意思決定を行う事を可能にしたい |
| 1.5.1: 誤識別率の最小化 | 誤識別をできるだけ少なくしたいということだけを目標としている |
| 1.5.2: 期待損失の最小化 | 単に誤識別を減らすよりも複雑化した決定理論について |
| 1.5.3: 棄却オプション | クラス分類が難しい場合は全て機械に選択を委ねる必要はない |
| 1.5.4: 推論と決定 | 決定問題を解く時に考えられる３種類のアプローチについて |
| 1.5.5: 回帰のための損失関数 | クラス分類だけでなく回帰でも決定問題を取り扱いたい |
| 1.6: 情報理論 | パターン認識や機械学習テクニックの発展にも有用である<br>情報理論に関する概念を導入する |
| 1.6.1: 相対エントロピーと相互情報量 | 未知の分布$p(\bm{x})$を近似的に$q(\bm{x})$でモデルする事を考える |
| **第２章: 確率分布** |  |
| 2.1: 二値変数 | コイン投げのような二値確率変数 $x = \{0, 1\}$ の場合を考える |
| 2.1.1: ベータ分布  | 二項分布のパラメータをベイズ主義的に推定する |
| 2.2: 多値変数 | 相互に排他的な$K$個の可能な状態のうち一つを取るような<br>離散変数を取り扱う |
| 2.2.1: ディリクレ分布 | 多項分布のパラメータをベイズ主義的に推定する |
| 2.3: ガウス分布 | 様々な場面で現れ、利用価値も様々なガウス分布を紹介 |
| 2.3.1: 条件付きガウス分布 | 多変量ガウス分布の重要な特性について考察する |
| 2.3.2: 周辺ガウス分布 | 周辺ガウス分布について考える |
| 2.3.3: ガウス変数に対するベイズの定理 | 以後の章で頻繁に現れる以下のような問題に対する<br>一般的な結果を算出しておく |
| 2.3.4: ガウス分布の最尤推定 | 多変量ガウス分布から観測値$\{x_n\}$が独立に得られたと仮定したデータ集合に対して、分布のパラメータを最尤推定法で推定する |
| 2.3.5: 逐次推定 | データ点を１度に１つずつ処理してそれを破棄することを考える |
| 2.3.6: ガウス分布に対するベイズ推論 | パラメータ上の事前分布を導入して、ベイズ主義的な扱い方を導く |
| 2.3.7: スチューデントの $\mathbf{t}$ 分布 | ガウス分布に関する応用分布をひとつ紹介する |
| 2.3.8: 周期変数 | ガウス分布を周期性に対応させる |
| 2.3.9: 混合ガウス分布 | 単一のガウス分布では捉えられない多峰性のあるデータを捉えたい |
| 2.4: 指数型分布族 | 今まで紹介してきた確率分布の共通点を述べる |
| 2.4.1: 最尤推定と十分統計量 | 一般的な指数型分布族のパラメータベクトルを最尤推定で推定する |
| 2.4.2: 共役事前分布 | 一般的な指数型分布族の共役な事前分布について考える |
| 2.4.3: 無情報事前分布 | 分布がどのような形状になるかについて知見があまり無い場合の対処法 |
| 2.5: ノンパラメトリック法 | 分布の形状を僅かにしか仮定しない手法について考える |
| 2.5.1: カーネル密度推定法 | 観測値の集合から未知の確率密度を推定したいとする |
| 2.5.2: 最近傍法 | カーネル密度推定法と似て非なる手法を考える |
| **第9章: 混合モデルとEM** |  |
| 9.1: K-means クラスタリング | 古典的なクラスタリング手法 K-means について |
| 9.1.1: 画像分割と画像圧縮 | 画像データに対して K-means を適用した結果とその考察 |
| 9.2: 混合ガウス分布 (Mixtures of Gaussians) | 離散的な潜在変数を用いた混合ガウス分布の定式化 |
| 9.2.1: 最尤推定 | 混合ガウス分布に従う観測データに対する最尤推定 |
| 9.2.2: 混合ガウス分布のEMアルゴリズム | EMアルゴリズムを用いた最尤解の推定 |
| 9.3: EMアルゴリズムのもう一つの解釈 | EMアルゴリズムに欠かせない潜在変数の役割について |
| 9.3.1: 混合ガウス分布再訪 | 混合ガウスモデルに対してEMアルゴリズムを適用する |
| 9.3.2: K-meansとの関連 | K-means と混合ガウス分布に対するEMアルゴリズムの関係性 |
| 9.3.3: 混合ベルヌーイ分布 | 混合ベルヌーイ分布に対するEMアルゴリズムの適用 |
| 9.3.4: ベイズ線形回帰に関するEMアルゴリズム | |
| 9.4: 一般のEMアルゴリズム | 具体例を踏まえた、EMアルゴリズムの基本 |

# 記号の説明
🧐: 僕が勉強する過程で理解するために自分なりに噛み砕いた表現や、疑問に思ったことを記載しています。(誤った内容がございましたら、ご指摘いただけると大変助かります🙇‍♂️)

# 更新情報
2024/09/17: 第９章追加
2023/07/03: 第２章追加
2023/06/30: 第１章追加
